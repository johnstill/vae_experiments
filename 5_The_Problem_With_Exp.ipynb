{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "import math\n",
    "import os\n",
    "from itertools import count, groupby\n",
    "from operator import itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.distributions import Bernoulli, Normal\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from pytorch_lightning import LightningModule, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaTCVAE(nn.Module):\n",
    "    \"\"\"beta-TC class VAE\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder : torch.nn.Module\n",
    "    decoder : torch.nn.Module\n",
    "        pytorch networks used to encode and decode to/from the latent variables\n",
    "    e : int\n",
    "        The output dimension of the encoder network\n",
    "    z : int\n",
    "        The input dimension of the decoder network\n",
    "    beta : int\n",
    "        Total Correlation weight term (default=1)\n",
    "    lamb : float in [0, 1]\n",
    "        Dimension wise KL term is (1 - lamb)\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, e, z, beta=1, lamb=0):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.e_dim = e\n",
    "        self.z_dim = z\n",
    "        self.beta = beta\n",
    "        self.lamb = lamb\n",
    "        # Learned Z Hyperparams\n",
    "        # Q: Why logvar and not stddev?\n",
    "        # A: https://stats.stackexchange.com/a/353222\n",
    "        self.mu = nn.Linear(e, z)\n",
    "        self.logvar = nn.Linear(e, z)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Hook for reparameterizing the outs of the encoder\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.mu(h)\n",
    "        std = torch.exp(0.5*self.logvar(h))\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + std*eps\n",
    "        return z, mu, std\n",
    "\n",
    "    def get_xdist(self, z):\n",
    "        \"\"\"Hook for customising interpretation of decoder output\"\"\"\n",
    "        return Bernoulli(logits=self.decoder(z))\n",
    "\n",
    "    def get_pdist(self, z):\n",
    "        \"\"\"Hook to customize prior distribution\"\"\"\n",
    "        return Normal(torch.zeros_like(z), torch.ones_like(z))\n",
    "\n",
    "    def get_qdist(self, mu, std):\n",
    "        \"\"\"Hook to customize construction of qdist from mean and stddev\"\"\"\n",
    "        return Normal(mu, std)\n",
    "\n",
    "    def forward(self, x, dataset_size):\n",
    "        \"\"\"Calculates the Evidence Lower Bound (ELBO) of the VAE on x\"\"\"\n",
    "        x_len = x.shape[0]\n",
    "        z, mu, std = self.encode(x)\n",
    "\n",
    "        # log(p(x))\n",
    "        xdist = self.get_xdist(z)\n",
    "        logpx = xdist.log_prob(x).view(x_len, -1).sum(1)\n",
    "\n",
    "        # log(p(z))\n",
    "        pdist = self.get_pdist(z)\n",
    "        logpz = pdist.log_prob(z).view(x_len, -1).sum(1)\n",
    "\n",
    "        # log(q(z|x))\n",
    "        qdist = self.get_qdist(mu, std)\n",
    "        logqz_condx = qdist.log_prob(z).view(x_len, -1).sum(1)\n",
    "\n",
    "        # Calculate matrix of shape (x_len, x_len, z_dim) which contains the\n",
    "        # log probability of each instance's latent variables under the\n",
    "        # distributions of every instance latent vars in the batch\n",
    "        qdist = qdist.expand((1, x_len, self.z_dim))\n",
    "        qzmat = qdist.log_prob(z.view(x_len, 1, self.z_dim))\n",
    "\n",
    "        # log(q(z)) via minibatch weighted sampling\n",
    "        logmn = math.log(dataset_size * x_len)\n",
    "        logqz = torch.logsumexp(qzmat.sum(2), dim=1) - logmn\n",
    "        logqz_prodmarginals = (torch.logsumexp(qzmat, dim=1) - logmn).sum(1)\n",
    "\n",
    "        # Calculate Modified ELBO:\n",
    "        # Basic ELBO is just logpx + logpz - logqz_condx\n",
    "        ix_code_mi = logqz_condx - logqz\n",
    "        total_corr = self.beta * (logqz - logqz_prodmarginals)\n",
    "        dimwise_kl = (1 - self.lamb) * (logqz_prodmarginals - logpz)\n",
    "        modified_elbo = logpx - ix_code_mi - total_corr - dimwise_kl\n",
    "        return modified_elbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /Users/stilljm/projects/johnstill/vae_experiments\n",
       "    Split: Train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNIST(os.getcwd(), train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 784]), 128)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = MNIST(os.getcwd(), transform=ToTensor())\n",
    "loader = DataLoader(data, batch_size=128)\n",
    "X, y = next(iter(loader))\n",
    "X = X.view(X.shape[0], -1)\n",
    "x_len = len(X)\n",
    "X.shape, x_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BetaTCVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=400, out_features=200, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=200, out_features=100, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=20, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=200, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=200, out_features=400, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=400, out_features=784, bias=True)\n",
       "  )\n",
       "  (mu): Linear(in_features=100, out_features=20, bias=True)\n",
       "  (logvar): Linear(in_features=100, out_features=20, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = 100\n",
    "z = 20\n",
    "encoder = nn.Sequential(nn.Linear(784, 400),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(400, 200),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(200, e),\n",
    "                        nn.ReLU())\n",
    "decoder = nn.Sequential(nn.Linear(z, e),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(e, 200),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(200, 400),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(400, 784))\n",
    "vae = BetaTCVAE(encoder, decoder, e, z)\n",
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 0 Batch 0 Loss: 600.1227416992188\n",
      "Epoch 0 Batch 100 Loss: 225.73538208007812\n",
      "Epoch 0 Batch 200 Loss: 191.56504821777344\n",
      "Epoch 0 Batch 300 Loss: 171.66139221191406\n",
      "Epoch 0 Batch 400 Loss: 228.4467010498047\n",
      "Epoch 1\n",
      "Epoch 1 Batch 0 Loss: 179.67726135253906\n",
      "Epoch 1 Batch 100 Loss: 172.30633544921875\n",
      "Epoch 1 Batch 200 Loss: 186.26449584960938\n",
      "Epoch 1 Batch 300 Loss: 160.93212890625\n",
      "Epoch 1 Batch 400 Loss: 186.82305908203125\n",
      "Epoch 2\n",
      "Epoch 2 Batch 0 Loss: 169.50994873046875\n",
      "Epoch 2 Batch 100 Loss: 201.45382690429688\n",
      "Epoch 2 Batch 200 Loss: 183.2902069091797\n",
      "Epoch 2 Batch 300 Loss: 161.8167724609375\n",
      "Epoch 2 Batch 400 Loss: 181.29129028320312\n",
      "Epoch 3\n",
      "Epoch 3 Batch 0 Loss: 173.8082733154297\n",
      "Epoch 3 Batch 100 Loss: 172.75070190429688\n",
      "Epoch 3 Batch 200 Loss: 181.92477416992188\n",
      "Epoch 3 Batch 300 Loss: 160.8887939453125\n",
      "Epoch 3 Batch 400 Loss: 186.173828125\n",
      "Epoch 4\n",
      "Epoch 4 Batch 0 Loss: 172.81275939941406\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-05df71479452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodified_elbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "\n",
    "for j in range(10):\n",
    "    print(f'Epoch {j}')\n",
    "    for i, (X, y) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x_len = X.shape[0]\n",
    "        X = X.view(x_len, -1)\n",
    "\n",
    "        z, mu, std = vae.encode(X)\n",
    "        ez = torch.exp(z)\n",
    "        assert not torch.isnan(ez).any()\n",
    "\n",
    "        xdist = vae.get_xdist(ez)\n",
    "        logpx = xdist.log_prob(X).sum(1)\n",
    "\n",
    "        pdist = vae.get_pdist(ez)\n",
    "        logpz = pdist.log_prob(ez).sum(1)\n",
    "\n",
    "        qdist = vae.get_qdist(mu, std)\n",
    "        logqz_condx = qdist.log_prob(z).sum(1)\n",
    "\n",
    "        qdist = qdist.expand((1, x_len, vae.z_dim))\n",
    "        qzmat = qdist.log_prob(ez.view(x_len, 1, vae.z_dim))\n",
    "\n",
    "        logmn = math.log(len(data) * x_len)\n",
    "        logqz = torch.logsumexp(qzmat.sum(2), dim=1) - logmn\n",
    "        logqz_prodmarginals = (torch.logsumexp(qzmat, dim=1) - logmn).sum(1)\n",
    "\n",
    "        ix_code_mi = logqz_condx - logqz\n",
    "        total_corr = vae.beta * (logqz - logqz_prodmarginals)\n",
    "        dimwise_kl = (1 - vae.lamb) * (logqz_prodmarginals - logpz)\n",
    "        modified_elbo = logpx - ix_code_mi - total_corr - dimwise_kl\n",
    "\n",
    "        loss = modified_elbo.mul(-1).mean()\n",
    "        assert not torch.isnan(loss).any()\n",
    "        assert not torch.isinf(loss).any()\n",
    "        if i %100 == 0:\n",
    "            print(f'Epoch {j} Batch {i} Loss: {loss.item()}')\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isnan(ez).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isinf(ez).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.5890e-25, 1.5919e-08, 3.1325e-13, 4.5740e-10, 1.9590e-15, 2.1965e-25,\n",
       "         2.9276e-25, 1.5055e-22, 9.7902e-04, 9.0113e-19, 3.2367e-26, 1.7909e-13,\n",
       "         1.3453e-22, 3.5180e-22, 3.8236e-16,        inf, 8.3145e-19, 6.9134e-20,\n",
       "         5.9167e-23, 2.0944e-24]], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ez[torch.isnan(logpx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([297.0267], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[torch.isinf(ez)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([inf], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(z[torch.isinf(ez)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.932459158195629e+128"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(297.0267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.0584e+02, -2.2263e+02, -1.5050e+02, -2.2141e+02, -1.4010e+02,\n",
       "        -1.7858e+02, -2.5226e+02, -2.0033e+02, -2.6512e+02, -2.0508e+02,\n",
       "        -1.9433e+02, -1.8068e+02, -1.4199e+03, -1.3812e+02, -1.6304e+02,\n",
       "        -2.2851e+02, -2.3150e+02, -1.6934e+02, -1.3970e+02, -2.4321e+02,\n",
       "        -2.0734e+02, -2.2168e+02, -1.9343e+02, -2.0837e+02, -2.9674e+02,\n",
       "        -1.7253e+02, -1.4250e+02, -1.9290e+02, -1.5223e+02, -2.5754e+02,\n",
       "        -2.2191e+02, -2.2269e+02, -2.4998e+02, -1.4117e+02, -2.0471e+02,\n",
       "        -5.0500e+07, -2.0817e+02, -1.3788e+02, -1.9334e+02, -2.4626e+02,\n",
       "                nan, -2.2713e+02, -1.4206e+02, -1.9521e+02, -1.2639e+02,\n",
       "        -2.4949e+02, -2.1696e+02, -1.5610e+02, -2.0589e+02, -1.5835e+02,\n",
       "        -1.1693e+02, -1.7149e+02, -2.4095e+02, -2.2080e+02, -1.9652e+02,\n",
       "        -1.1750e+02, -1.3911e+02, -1.6507e+02, -3.6388e+02, -1.8240e+02,\n",
       "        -1.6396e+02, -2.2006e+02, -2.1524e+02, -2.5575e+02, -1.6065e+02,\n",
       "        -1.4846e+02, -1.8503e+02, -1.8924e+02, -1.4736e+02, -2.1397e+02,\n",
       "        -2.2399e+02, -9.0992e+04, -2.8812e+02, -1.8706e+02, -1.6094e+02,\n",
       "        -1.2241e+02, -2.3278e+02, -1.7680e+02, -1.7887e+02, -1.7813e+02,\n",
       "        -2.1351e+02, -2.6804e+02, -2.2066e+02, -1.2521e+02, -1.7515e+02,\n",
       "        -1.8920e+02, -2.4197e+02, -2.6436e+02, -1.2332e+02, -1.5953e+02,\n",
       "        -1.5451e+02, -1.6964e+02, -1.2035e+02, -2.3584e+02, -2.2710e+02,\n",
       "        -1.9143e+02, -2.4069e+02, -2.4254e+02, -1.4954e+02, -1.4649e+02,\n",
       "        -1.8100e+02, -1.8673e+02, -1.5908e+02, -2.1860e+02, -1.6945e+02,\n",
       "        -1.9047e+02, -1.8024e+02, -1.8328e+02, -1.7302e+02, -3.6816e+02,\n",
       "        -2.3429e+02, -2.3558e+02, -1.9933e+02, -1.1351e+02, -2.4673e+02,\n",
       "        -1.7355e+02, -1.6391e+02, -1.6392e+02, -2.9205e+02, -1.5163e+02,\n",
       "        -1.3913e+02, -2.0036e+02, -2.3397e+02, -1.9819e+02, -2.5530e+02,\n",
       "        -2.0060e+02, -2.6858e+02, -1.7750e+02], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdist.log_prob(X).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ez.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-57.4084], grad_fn=<IndexBackward>),\n",
       " tensor([99.9527], grad_fn=<IndexBackward>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_idx = torch.isinf(ez)\n",
    "mu[inf_idx], std[inf_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
